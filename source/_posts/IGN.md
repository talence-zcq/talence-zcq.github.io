---
title: 什么是 Invariant Graph Network(IGN)和EGN(Equivariant graph network)
date: 2023-05-28 10:16:30
tags: 图神经网络表达能力
mathjax: true
---

# 一句话解释 Invariant Graph Network(IGN)

IGN简单来说就是一类用来处理图数据结构的网络，它能够对**节点的位置信息变化**（邻接矩阵的行）和**节点的特征位置变化**（邻接矩阵的列）不敏感。

如果不能理解上面这段话的意思，我们可以看看下面这一段例子：

![](https://zcq-hexo.oss-cn-hangzhou.aliyuncs.com/img/IGN-pic1.png)

假设我们的输入是一只猫，但是**猫的位置**和**猫的颜色**都会随着不同的输入发生变化，但是我们期望网络的输出都不会改变，即输入一张猫的图片（可能在图片中的位置和颜色不同），输出一个值是猫的标签。

用公式具体表示就是：

$$
L(T \times x) = L(x) 
$$

对于所有的x都成立，T表示的是对输入数据的变化操作，如打乱顺序，L是具有Invariant特性的线性层

这样做的好处就是，我们会减少很多学习参数。如果用MLP（一个可以模拟任何函数的处理器）去学习输入的话，那么我们只能学习到一种模式下的输入特征，我们稍微变化一下输入，MLP的参数就需要重新学习新的模式了。

# 一句话解释Equivariant Graph Network(EGN)

EGN跟IGN恰恰相反，IGN的输出是不会随着节点输入的变化而变化，而EGN的节点输出和节点输入必须是等变的。如果一个节点在输入端的1、3行位置进行了对换，那么这个图数据在经过EGN网络后，他的输出也要进行1、3行的对换。

同样沿用上面的例子：

![](https://zcq-hexo.oss-cn-hangzhou.aliyuncs.com/img/EGN-cat.png)

假设我们输入了一只猫，如上图左上角所示，他的输出是一只边缘锐化过的猫。但是，我们将猫的位置进行变化后，如上图右上角所示，他的输出也是一只边缘锐化的猫。而且两个输入和两个输出相比，他们的**位置变化都是一致的**（从右半边移到了左半边）。

用公式具体表示就是：

$$
L(T \times x) = T \times L(x)
$$

对于所有的x都成立，T表示的是对输入数据的变化操作，如打乱顺序，L是具有Equivariant特性的线性层

# 一种极具表达能力的框架范式
在这里我们将用上面两种类型的网络，组合出一种极具实战能力的范式，这种范式在卷积神经网络领域取得了极大的成功。如下图所示：

![](https://zcq-hexo.oss-cn-hangzhou.aliyuncs.com/img/%E6%9E%81%E5%85%B7%E8%A1%A8%E8%BE%BE%E8%83%BD%E5%8A%9B%E7%9A%84%E8%8C%83%E5%BC%8F.png)

上面的网络结构由多个等变网络(L)组成，之后跟上一个不变网络(H)和一个全连接层(M)。用公式具体表示就是：

$$
f(x) = M \cdot H \cdot L_k \cdot ... \cdot L_1(x)
$$

卷积神经网络中的卷积核代表的就是等变神经网络，之后在进行一些等变网络和全连接层，卷积神经网络就可以很好的处理图像类型的数据。

假如我们把输入换成图结构数据类型，也存在一系列的等变网络和不变网络层，使他能够组成上面那样类型的范式。于是，我们将他统称为**IGN**（一种宽泛的定义）

因为$ f(x) = M \cdot H \cdot L_k \cdot ... \cdot L_1(x)$表达式，总体上还是满足$f(p \cdot X) = f(X)$的，p代表的是对图数据的打乱操作。

但是图类型的数据结构输入，我们需要具体详细的定义，因为之前的图数据结构分成多个张量进行表示，已经不适用于该理论的研究。
# 一种表示图的数据结构
之前，大部分文章表示图数据G, 都是用(V,E)。点的特征用$F^V \in R^{n \times d}$，边的特征用$F^E \in R^{m \times d}$。其n和m分别代表点的数量和边的数量，d代表点的特征维度和边的特征维度（在这里他们是一样的，都是d维）。并且，我们还需要一个邻接矩阵$A \in R^{n \times n}$，代表两个点是否存在连接关系。

但是，在接下来，我们要把上面所有的关于图的数据，都塞进一个三维的张量X中，$X \in R^{n^2 \times (2d + 1)}$，如下图所示：

![](https://zcq-hexo.oss-cn-hangzhou.aliyuncs.com/img/graph_tensor.png)

首先最开始的n*n个切片，$X_{:,:,1}$,代表着之前的邻接矩阵。

之后的d个通道，$X_{:,:,2:d+1}$,代表着点的特征。

最后的d个通道，$X_{:,:,d+2:2d+1}$,代表着边的特征。

于是，我们就通过一个三维张量，将之前的传统图数据整合到了一起，形成了一个新的数据结构。

我们在把它的形式一般化，如下所示：
$$
X \in R^{n^k \times c}
$$

上述式子代表着一条边，可以至多连接k个节点，也就是我们所说的超图形式。而每一个点，每一个边的特征，我们都整合以通道数为c的向量表示。我们可以在里面进行我们所定义的切分，如前$c \times d$个通道代表着点的特征，前$(c+1)*d$个通道代表着边特征等。

# 什么是k-IGN

首先我们把目光投向一个线性变化，$L:R^{n^k} -> R^{n^l}$。根据上述小节，这是将一组k个节点的图映射到一组l个节点的图上。

所以他们的变化线性层就应该是$L \in R^{n^{k+l}}$。如果不理解，我们可以把它降维。假设线性变化为$L:R^k->R^l$，那么线性变化层就应该是$L \in R^{k \times l}$了。

于是，我们把输入维度为$R^{n^k}$的映射变化称为k-IGN。
# 图不变网络（IGN）和图等变网络(EGN)的特性

已经了解了IGN和EGN的简单定义，以及新的图数据结构。我们就可以用以上先验知识去理论推导一些神奇的特性。具体特性将在本博客的其他文章中进行探讨。

# 参考文献
Maron H, Ben-Hamu H, Shamir N, et al. Invariant and equivariant graph networks[J]. arXiv preprint arXiv:1812.09902, 2018.
