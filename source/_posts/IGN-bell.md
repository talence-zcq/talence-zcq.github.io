---
title:  Invariant Graph Network中的Bell数特性
date: 2023-06-02 15:23:50
tags: 图神经网络表达能力
mathjax: true
---

# 什么是Bell数

Bell数的定义：第n个Bell数表示集合{1,2,3,...,n}的划分方案数，即：$B_0$ = 1;

例如， $B_3$ = 5 是因为3个元素的集合有5种划分方法,具体如下所示：

【{a},{b},{c}】、【{a},{b,c}】、【{b},{a,c}】、【{c},{a,b}】、【{a,b,c}】

贝尔数有递推公式：

$$
B_{n+1}= \sum_{k=0}^n\binom{n}{k}B_k
$$

看不懂递推没关系，我们这里只需要知道Bell数是描述什么的就行了。

# IGN的bell数特性

我们首先定义一个线性等变层的公式：

$$
L(p \cdot X) = p \cdot L(X)
$$

它可以通过证明1，来得到以下公式：

$$
p \cdot L = L
$$

直观的来看就是，在数据上的变化和等变线性层上的变化必须是一致的。就比如说，我的扰动集p将第一个点和第三个点调换了下位置，那么等变层L的第一行的参数和第三行的参数也必须是一致的。

作者将他称为**固定点等式（fixed point equations）**。因为在$L:R^{n^k}->R^{n^l}$中，这个线性等变层（或是线性不变层）$L \in R^{n^{k+l}}$的某些参数，是随着扰动集合(p)而固定的。

我们要如何求解这个式子$ p \cdot L = L $,需要看证明2，我们这里直接说他得到的结论，就是本文要提到的有关bell数的结论。

在这里，方便结论的理解，我们把l=k=2。他就是把一个边包含两个节点的矩阵图，映射到另外一个边包含两个节点的矩阵图中。

如果我们的p=(12),意思就是将第一行的节点特征与第二行的节点特征调换顺序，其他节点特征都保持不变。那么通过上述等式$p\cdot L = L$我们会得到，$L_{1,1,1,1} = p \cdot L_{1,1,1,1} = L_{2,2,2,2}$
同样的，我们考虑p=(23)。那么我们同样会得到：$L_{2,2,2,2}=L_{3,3,3,3}$

保持这样的的扰动集合，我们可以得到以下范式，$L_{i,i,i,i}$的值都是一样的。$L_{i,i,j,s}$，其中j≠s≠i，这样的范式又是一样的。所以这样的等价集合范式在l=k=2的情况下等于15个。

也就是符合我们上文所定义的Bell(4)=15。

![](https://zcq-hexo.oss-cn-hangzhou.aliyuncs.com/img/bell(4)%E6%AD%A3%E4%BA%A4%E5%9F%BA.png)

作者把这15个线性等变（或是线性不变）层看成正交基，因为他们是具有相同值的等价类。为了更具体的展示，我们假设n=5（n是点的数量），l=k=2。

然后作者将R^{n^2}进行列的堆叠，最终组成了25*25大小的正方形矩阵。然后给定一种范式α，如果L的下标满足这个α范式的话，就显示白色，如果不满足，就显示黑色。下图是这15个正交基的可视化展示结果：

![](https://zcq-hexo.oss-cn-hangzhou.aliyuncs.com/img/%E6%AD%A3%E4%BA%A4%E5%9F%BA%E5%8F%AF%E8%A7%86%E5%8C%96.png)

然后作者对这15个正交基的系数进行学习，最终学习到一个适合的线性等变层（或是线性不变层）。具体的学习公式如下：

$$
L = \sum_α w_α B^α
$$
其中，$B^α$代表这个正交基的0，1矩阵，就像可视化展示的那样。

# 证明IGN的bell数特性

# 参考文献
Maron H, Ben-Hamu H, Shamir N, et al. Invariant and equivariant graph networks[J]. arXiv preprint arXiv:1812.09902, 2018.