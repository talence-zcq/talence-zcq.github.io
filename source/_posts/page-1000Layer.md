---
title: 论文笔记<二>  Training Graph Neural Networks with 1000 Layers
date: 2022-11-28 22:49:29
tags: 图神经网络
categories: 研0
mathjax: true
---

# 基本信息
### 作者
 **Guohao Li**  Intel Labs <guohao.li@kaust.edu.sa>  
 **Matthias Muller**  
 **Bernard Ghanem**   
 **Vladlen Koltun**  

 ### 发布会议
 Accepted at ICML 2021
 
 ### 数据集
| 数据集(RevGNN-Wide)| ROC-AUC | Mem | Params|
| ----------- | ----------- | ----------- | ----------- |
| ogbn-proteins | $88.24±0.15$ | $7.91$ | $68.47M$ |
| ogbn-arxiv| $74.05 ± 0.11$  | $8.49$ | $3.88M$ |  


# 创新点概要

### 特征矩阵三合一  
作者认为点和边在特征矩阵上会有一定联系，为了捕获每个维度的特征。因此将点特征矩阵（X∈N×D）、邻接矩阵（A∈N×N）、边特征矩阵（E∈M×F）映射到 X' 特征矩阵上（N×D），公式如下:
$$
f_w = X×A×U \rightarrow X'
$$


<!---黄色-->
<div class="wy">
  <div class="t">
    注意
  </div>
  <div class="c">
    这里的X是每一层网络的输入。A和E的特征矩阵在每一层都是一致的，都是初始值。
  </div>
</div>
<br/>

 ### 分组可逆卷积
 #### 分组部分
作者把X的特征拆分成了C份<$X_1$,$X_2$...$X_c$>。最终目的是要使得<$X_1$,$X_2$...$X_c$>$\rightarrow$<$X_1'$,$X_2'$...$X_c'$>，具体操作如下公式所示：
$$
X_0' = \sum_{i=2}^{C}X_i
$$

$$
X_i' = f_{w_i}(X_{i-1}',A,U)+X_i, i\in \{1,...,C\}
$$
 #### 可逆部分
 为了节省内存消耗量，以训练更深的网络和保存更多的参数，作者用了可逆求导。这个方法的好处就是**不用保存每一层的隐藏特征**，而是**只用保存最后一层的隐藏特征**即可。并用最后一层的隐藏特征往回运算求得与原来参数的梯度，从而改进权重，具体操作如下公式所示：

$$
X_i = X_i'-f_{w_i}(X_{i-1}',A,U),i\in\{2,...,C\}
$$

$$
X_0'=\sum_{i=2}^CX_i
$$

$$
X_1 = X_1'-f_{w1}(X_0',A,U)
$$

### 归一化和随即丢弃
然而作者发现，在他们的网络中，归一化和随机丢弃层是不可缺少的，于是在每个$X_{i-1}'$变为下一层的$X_i$时，会经历以下公式：
$$
\hat{X} =Dropout(ReLU(Norm(X_{i-1}')))
$$

$$
\widetilde{X} = GraphConv(\hat{X}_i,A,U)
$$

 ### 权重共享
作者为了简约空间存储，将内存储存大小与网络深度独立，作者将网络的每一层的权重都进行了共享。

<!---绿色--->
<div class="wg">
  <div class="t">
    提示
  </div>
  <div class="c">
    若是不分组卷积，那么每一层就一个 W ，且每一个 W 都是相同的。若是分组卷积，那么每一层就会有C个 W，每一层的对应那组 W 是相同的
  </div>
</div>
<br/>

 ### 状态平衡

 因为权重都是共享的，为了使图圣经网络达到一个稳定状态。作者设立了一个$Z$，对于每一层每一个每一时刻的点，都会输入去对$Z$进行更新。公式整体如下：

$$
Z^* = f_w^{DEQ}(Z^*,X,A,U)
$$

公式的详细部分 $f_w^{DEQ}$ 如下：

$$
Z'=GraphConv(Z_{in},A,U)
$$

$$
Z''= Norm(Z'+X)
$$

$$
Z''' = GraphConv(Dropout(ReLU(Z'')),A,U)
$$

$$
Z_o = Norm(ReLU(Z'''+Z'))
$$
 
# 个人心得
1.作者很好的考虑到了在图神经网络领域中，层数对训练的影响，为以后自己做研究打下补丁。
2